{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aabafca-8129-4943-b865-d5e897637253",
   "metadata": {},
   "source": [
    "![image](car.jpeg)\n",
    "\n",
    "**Car-ing is sharing**, an auto dealership company for car sales and rental, is taking their services to the next level thanks to **Large Language Models (LLMs)**.\n",
    "\n",
    "As their newly recruited AI and NLP developer, you've been asked to prototype a chatbot app with multiple functionalities that not only assist customers but also provide support to human agents in the company.\n",
    "\n",
    "The solution should receive textual prompts and use a variety of pre-trained Hugging Face LLMs to respond to a series of tasks, e.g. classifying the sentiment in a car’s text review, answering a customer question, summarizing or translating text, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c0004-e8c7-4539-967d-0c32167ae540",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "In order to complete the project you may wish to install some Hugging Face libraries such as `transformers` and `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325a4c0-ceb3-4b66-acd2-5eadcefe3a63",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 22459,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1744360302105,
    "lastExecutedByKernel": "6f104d30-40ba-4ed2-a615-4a1d9c70340b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!pip install -q transformers\n!pip install -q evaluate==0.4.0\n!pip install -q datasets==2.10.0\n!pip install -q sentencepiece==0.1.97\n!pip install -q openai\n!pip install -q tenacity\n\n\nfrom transformers import logging",
    "outputsMetadata": {
     "0": {
      "height": 553,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q transformers\n",
    "# !pip install -q evaluate==0.4.0\n",
    "# !pip install -q datasets==2.10.0\n",
    "# !pip install -q sentencepiece==0.1.97\n",
    "# !pip install -q openai\n",
    "# !pip install -q tenacity\n",
    "\n",
    "\n",
    "#from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0fad55-e44d-4893-ab91-f083358faeec",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 5302,
    "lastExecutedAt": 1744360307408,
    "lastExecutedByKernel": "6f104d30-40ba-4ed2-a615-4a1d9c70340b",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import os\nimport sys\nimport importlib.metadata\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom openai import OpenAI\nimport logging\nfrom tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\nimport openai\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\nimport evaluate",
    "outputsMetadata": {
     "0": {
      "height": 416,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stormbird/Downloads/HF_model_review/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib.metadata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "import openai\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "760f0eab-5cc9-45ff-bd85-fbfb0b4d4d5e",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "4": {
      "height": 38,
      "type": "stream"
     },
     "5": {
      "height": 616,
      "type": "stream"
     },
     "8": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9294)\n",
      "\n",
      "Review: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.8654)\n",
      "\n",
      "Review: My first foreign car. Love it, I would buy another.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9995)\n",
      "\n",
      "Review: I've come across numerous reviews praising the Rogue, and I genuinely feel like I might be missing something. It's only been a week since I got the car, and I am genuinely disappointed. I truly wish I could return it. My main concern revolves around what I see as a significant design flaw (which I believe also exists in the Murano, though that wasn't much better and considerably pricier). The rear windshield is just too small. The headrests in the back seat obstruct the sides of the rearview window. This \"Crossover\" feels more like a cheaply made compact car. My other vehicle is a Sonata, and it provides a significantly quieter and smoother ride. I did not anticipate this car to ride so roughly; my 2006 Pathfinder had a smoother ride! I would rate this car a 5 all around.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9935)\n",
      "\n",
      "Review: I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9987)\n",
      "\n",
      "Accuracy: 0.8\n",
      "F1 result: 0.8571428571428571\n",
      "Accuracy: 0.8\n",
      "F1 result: 0.8571428571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your input_length: 365 is bigger than 0.9 * max_length: 27. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 365 is bigger than 0.9 * max_length: 27. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model translation:\n",
      "Estoy muy satisfecho con mi 2014 Nissan NV SL. Uso esta furgoneta para mis entregas de negocios y uso personal.\n",
      "Spanish translation references:\n",
      "['Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.', 'Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta furgoneta para mis entregas comerciales y uso personal.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 7.41MB/s]\n",
      "\n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 15.2MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6022774485691839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Answer:  ride quality, reliability\n",
      "Original text:\n",
      "I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text:\n",
      "the Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. I have hauled 12 bags of mulch in the back with the seats down and could have held more.\n"
     ]
    }
   ],
   "source": [
    "# Mostly for testing if packages are working okay & huggingface utilities\n",
    "#Load the car reviews dataset\n",
    "file_path = \"data/car_reviews.csv\"\n",
    "df = pd.read_csv(file_path, delimiter=\";\")\n",
    "\n",
    "# Put the car reviews and their associated sentiment labels in two lists\n",
    "reviews = df['Review'].tolist()\n",
    "real_labels = df['Class'].tolist()\n",
    "\n",
    "\n",
    "# Instruction 1: sentiment classification\n",
    "\n",
    "# Load a sentiment analysis LLM into a pipeline\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Perform inference on the car reviews and display prediction results\n",
    "predicted_labels = classifier(reviews)\n",
    "for review, prediction, label in zip(reviews, predicted_labels, real_labels):\n",
    "    print(f\"Review: {review}\\nActual Sentiment: {label}\\nPredicted Sentiment: {prediction['label']} (Confidence: {prediction['score']:.4f})\\n\")\n",
    "\n",
    "# Load accuracy and F1 score metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Map categorical sentiment labels into integer labels\n",
    "references = [1 if label == \"POSITIVE\" else 0 for label in real_labels]\n",
    "predictions = [1 if label['label'] == \"POSITIVE\" else 0 for label in predicted_labels]\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_result_dict = accuracy.compute(references=references, predictions=predictions)\n",
    "accuracy_result = accuracy_result_dict['accuracy']\n",
    "\n",
    "# Fix for f1 calculation - handle the result safely whether it's a float or array\n",
    "from sklearn.metrics import f1_score\n",
    "f1_result = f1_score(references, predictions)\n",
    "print(f\"Accuracy: {accuracy_result}\")\n",
    "print(f\"F1 result: {f1_result}\")\n",
    "\n",
    "\n",
    "# Instruction 2: Translation\n",
    "\n",
    "# Load translation LLM into a pipeline and translate car review\n",
    "first_review = reviews[0]\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "translated_review = translator(first_review, max_length=27)[0]['translation_text']\n",
    "print(f\"Model translation:\\n{translated_review}\")\n",
    "\n",
    "# Load reference translations from file\n",
    "with open(\"data/reference_translations.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "references = [line.strip() for line in lines]\n",
    "print(f\"Spanish translation references:\\n{references}\")\n",
    "\n",
    "# Load and calculate BLEU score metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(predictions=[translated_review], references=[references])\n",
    "print(bleu_score['bleu'])\n",
    "\n",
    "\n",
    "# Instruction 3: extractive QA\n",
    "# Instantiate model and tokenizer\n",
    "model_ckp = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckp)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckp)\n",
    "\n",
    "# Define context and question, and tokenize them\n",
    "context = reviews[1]\n",
    "print(f\"Context:\\n{context}\")\n",
    "question = \"What did he like about the brand?\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Perform inference and extract answer from raw outputs\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "# Decode and show answer\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)\n",
    "\n",
    "\n",
    "# Instruction 4\n",
    "\n",
    "# Get original text to summarize upon car review\n",
    "text_to_summarize = reviews[-1]\n",
    "print(f\"Original text:\\n{text_to_summarize}\")\n",
    "\n",
    "# Load summarization pipeline and perform inference\n",
    "model_name = \"cnicu/t5-small-booksum\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "outputs = summarizer(text_to_summarize, max_length=53)\n",
    "summarized_text = outputs[0]['summary_text']\n",
    "print(f\"Summarized text:\\n{summarized_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1729b26-2e8b-4c9f-90f8-6ef89c72230e",
   "metadata": {
    "executionCancelledAt": 1744360323604,
    "executionTime": 49,
    "lastExecutedAt": 1744283056480,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# check out the versions of these packages in \nimport sys\nimport importlib.metadata\n\n# Check Python version\nprint(f\"Python version: {sys.version}\")\n\n# List of packages to check (use distribution names)\npackages = [\n    \"numpy\",\n    \"pandas\",\n    \"openai\",\n    \"tenacity\",\n    \"transformers\"\n]\n\n# Check package versions\nprint(\"\\nPackage versions:\")\nfor package_name in packages:\n    try:\n        version = importlib.metadata.version(package_name)\n        print(f\"- {package_name}: {version}\")\n    except importlib.metadata.PackageNotFoundError:\n        print(f\"- {package_name}: Not found (or version not available via importlib.metadata)\")",
    "outputsMetadata": {
     "0": {
      "height": 185,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0]\n",
      "\n",
      "Package versions:\n",
      "- numpy: 2.2.4\n",
      "- pandas: 2.2.3\n",
      "- openai: 1.72.0\n",
      "- tenacity: 9.1.2\n",
      "- transformers: 4.51.2\n"
     ]
    }
   ],
   "source": [
    "# Check Python version\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# List of packages to check (use distribution names)\n",
    "packages = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"openai\",\n",
    "    \"tenacity\",\n",
    "    \"transformers\"\n",
    "]\n",
    "\n",
    "# Check package versions\n",
    "print(\"\\nPackage versions:\")\n",
    "for package_name in packages:\n",
    "    try:\n",
    "        version = importlib.metadata.version(package_name)\n",
    "        print(f\"- {package_name}: {version}\")\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        print(f\"- {package_name}: Not found (or version not available via importlib.metadata)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53969d66-8847-4e0c-a721-f80fe9b3d6f9",
   "metadata": {
    "executionCancelledAt": 1744360323606,
    "executionTime": 8,
    "lastExecutedAt": 1744283083457,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# load env variable\ntoken = os.getenv(\"HF_TOKEN\")",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "# load env variable\n",
    "token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844a311-9086-4da7-a460-18ca49876a41",
   "metadata": {
    "executionCancelledAt": 1744360323607,
    "executionTime": 317,
    "lastExecutedAt": 1744282849921,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "client = OpenAI(\n\tbase_url=\"https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-1.5B-Instruct/v1\",\n\tapi_key=os.getenv(\"HF_TOKEN\")\n)\n\n\ndef run_chatgpt(prompt, client, model=\"Qwen/Qwen2.5-1.5B-Instruct\"):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.0,\n        seed=123,\n    )\n    return response.choices[0].message.content\n\n\nprompt = f\"Respond with 'hello world' if you got this message.\"\nrun_chatgpt(prompt, client)"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "\tbase_url=\"https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-1.5B-Instruct/v1\",\n",
    "\tapi_key=token\n",
    ")\n",
    "\n",
    "\n",
    "def run_chatgpt(prompt, client, model=\"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0,\n",
    "        seed=123,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "prompt = f\"Respond with 'hello world' if you got this message.\"\n",
    "run_chatgpt(prompt, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d997900a-847d-4713-84b5-bb1fbef3d96b",
   "metadata": {
    "executionCancelledAt": 1744360323608,
    "executionTime": 978,
    "lastExecutedAt": 1744281753363,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Set up logging\n#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n#logger = logging.getLogger(__name__)\n\n\n@retry(\n    retry=retry_if_exception_type((openai.RateLimitError, openai.APIConnectionError, openai.APITimeoutError)),\n    stop=stop_after_attempt(5),\n    wait=wait_exponential(multiplier=1, min=2, max=60),\n    before_sleep=lambda retry_state: logger.warning(\n        f\"Retry attempt {retry_state.attempt_number} after error: {retry_state.outcome.exception()}\"\n    )\n)\ndef run_hfgpt(system_prompt, user_prompt, client, model=\"Qwen/Qwen2.5-1.5B-Instruct\"):\n    \"\"\"\n    Run a query against an HuggingFace model with retry logic and error handling.\n    \n    Args:\n        system_prompt: The system instruction for the model\n        user_prompt: The user query to process\n        client: The OpenAI-compatible client\n        model: Model identifier to use\n        \n    Returns:\n        The model's response text\n    \"\"\"\n    from openai import OpenAI\n\n    client = OpenAI(\n        base_url=\"https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-1.5B-Instruct/v1\",\n        api_key=token\n    )\n    try:\n        print(f\"Sending request to model: {model}\")\n        response = client.chat.completions.create(\n            model=model,\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=0.0,\n            seed=123,\n        )\n        return response.choices[0].message.content\n    except openai.RateLimitError as e:\n        print(f\"Rate limit exceeded: {e}\")\n        raise\n    except openai.APIConnectionError as e:\n        print(f\"API connection error: {e}\")\n        raise\n    except openai.APITimeoutError as e:\n        print(f\"API timeout error: {e}\")\n        raise\n    except openai.BadRequestError as e:\n        print(f\"Bad request error: {e}\")\n        # Don't retry bad requests as they're likely client errors\n        raise\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        raise\n\nsystem_prompt = \"You are a helpful, harmless and honest assistant\" # constitutional ai prompt\nuser_prompt = f\"Respond with 'hello world' if you got this message.\"\n\nresult = run_hfgpt(system_prompt, user_prompt, client)\nprint(result)",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@retry(\n",
    "    retry=retry_if_exception_type((openai.RateLimitError, openai.APIConnectionError, openai.APITimeoutError)),\n",
    "    stop=stop_after_attempt(5),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=60),\n",
    "    before_sleep=lambda retry_state: logger.warning(\n",
    "        f\"Retry attempt {retry_state.attempt_number} after error: {retry_state.outcome.exception()}\"\n",
    "    )\n",
    ")\n",
    "def run_hfgpt(system_prompt, user_prompt, client, model=\"Qwen/Qwen2.5-1.5B-Instruct\"):\n",
    "    \"\"\"\n",
    "    Run a query against an HuggingFace model with retry logic and error handling.\n",
    "\n",
    "    Args:\n",
    "        system_prompt: The system instruction for the model\n",
    "        user_prompt: The user query to process\n",
    "        client: The OpenAI-compatible client\n",
    "        model: Model identifier to use\n",
    "\n",
    "    Returns:\n",
    "        The model's response text\n",
    "    \"\"\"\n",
    "    # Check if the client is initialized\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://router.huggingface.co/hf-inference/models/Qwen/Qwen2.5-1.5B-Instruct/v1\",\n",
    "        api_key=token\n",
    "    )\n",
    "    try:\n",
    "        print(f\"Sending request to model: {model}\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            seed=123,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except openai.RateLimitError as e:\n",
    "        print(f\"Rate limit exceeded: {e}\")\n",
    "        raise\n",
    "    except openai.APIConnectionError as e:\n",
    "        print(f\"API connection error: {e}\")\n",
    "        raise\n",
    "    except openai.APITimeoutError as e:\n",
    "        print(f\"API timeout error: {e}\")\n",
    "        raise\n",
    "    except openai.BadRequestError as e:\n",
    "        print(f\"Bad request error: {e}\")\n",
    "        # Don't retry bad requests as they're likely client errors\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        raise\n",
    "\n",
    "system_prompt = \"You are a helpful, harmless and honest assistant\" # constitutional ai prompt\n",
    "user_prompt = f\"Respond with 'hello world' if you got this message.\"\n",
    "\n",
    "result = run_hfgpt(system_prompt, user_prompt, client)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9c1ca-9fae-4ee2-8bcf-744a1138f06a",
   "metadata": {},
   "source": [
    "The model API is working and we have a function that concentrates the API best practices to enable us todo the consequent tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b53c93-aa1f-480f-bd48-8a50aaf1986a",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6777d5b-1346-4a1a-aab9-b963895213b8",
   "metadata": {},
   "source": [
    "We will load the data with pandas and craft a prompt to analyze the car reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c4f79-15ba-49b1-9c96-f3f1d54a31fc",
   "metadata": {
    "executionCancelledAt": 1744360323609,
    "executionTime": 176,
    "lastExecutedAt": 1744282850097,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!head -n 30 data/car_reviews.csv",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "!head -n 30 data/car_reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8eed49-6281-48b0-aff1-a1b3da4b6c62",
   "metadata": {
    "executionCancelledAt": 1744360323611,
    "executionTime": 148,
    "lastExecutedAt": 1744282850247,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "!head data/reference_translations.txt",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "!head data/reference_translations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24c4f5-5ec5-4323-8954-50126311b7a9",
   "metadata": {},
   "source": [
    "## Loading data up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a77b03-97c5-439a-b7b0-53f34e0d3729",
   "metadata": {
    "executionCancelledAt": 1744360323612,
    "executionTime": 23,
    "lastExecutedAt": 1744283093024,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "car_reviews = pd.read_csv(\"data/car_reviews.csv\", delimiter=\";\")"
   },
   "outputs": [],
   "source": [
    "car_reviews = pd.read_csv(\"data/car_reviews.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96bf034-6ee2-4dfc-9c39-97b08893c8c0",
   "metadata": {
    "executionCancelledAt": 1744360323613,
    "executionTime": 51,
    "lastExecutedAt": 1744283114408,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def read_file(filepath, encoding='utf-8', chunk_size=None):\n    \"\"\"\n    Efficiently read file content with proper resource management and optional chunking.\n    \n    Args:\n        filepath: Path to the file\n        encoding: File encoding (default: utf-8)\n        chunk_size: Size of chunks to read (None reads entire file at once)\n        \n    Returns:\n        File content as string\n    \"\"\"\n    try:\n        with open(filepath, 'r', encoding=encoding) as file:\n            if chunk_size:\n                # Process large files in chunks to reduce memory usage\n                content = []\n                while chunk := file.read(chunk_size):\n                    content.append(chunk)\n                return ''.join(content)\n            else:\n                # Read entire file at once for small files\n                return file.read()\n    except FileNotFoundError:\n        print(f\"Error: File '{filepath}' not found\")\n        return None\n    except PermissionError:\n        print(f\"Error: No permission to read '{filepath}'\")\n        return None\n    except UnicodeDecodeError:\n        print(f\"Error: File '{filepath}' has encoding issues. Try different encoding.\")\n        return None\n    except Exception as e:\n        print(f\"Error reading file '{filepath}': {e}\")\n        return None\n\n# Example usage\nfilepath = 'data/reference_translations.txt'\nfile_content = read_file(filepath)\n\nif file_content is not None:\n    print(file_content)\n    print(f\"The data type of this is: {type(file_content)}\")",
    "outputsMetadata": {
     "0": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.\n",
      "Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta furgoneta para mis entregas comerciales y uso personal.\n",
      "The data type of this is: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "def read_file(filepath, encoding='utf-8', chunk_size=None):\n",
    "    \"\"\"\n",
    "    Efficiently read file content with proper resource management and optional chunking.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the file\n",
    "        encoding: File encoding (default: utf-8)\n",
    "        chunk_size: Size of chunks to read (None reads entire file at once)\n",
    "\n",
    "    Returns:\n",
    "        File content as string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding=encoding) as file:\n",
    "            if chunk_size:\n",
    "                # Process large files in chunks to reduce memory usage\n",
    "                content = []\n",
    "                while chunk := file.read(chunk_size):\n",
    "                    content.append(chunk)\n",
    "                return ''.join(content)\n",
    "            else:\n",
    "                # Read entire file at once for small files\n",
    "                return file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found\")\n",
    "        return None\n",
    "    except PermissionError:\n",
    "        print(f\"Error: No permission to read '{filepath}'\")\n",
    "        return None\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Error: File '{filepath}' has encoding issues. Try different encoding.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "filepath = 'data/reference_translations.txt'\n",
    "file_content = read_file(filepath)\n",
    "\n",
    "if file_content is not None:\n",
    "    print(file_content)\n",
    "    print(f\"The data type of this is: {type(file_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "711bd6ff-a3f1-4c7b-a5c8-ef1602c03e47",
   "metadata": {
    "chartConfig": {
     "bar": {
      "hasRoundedCorners": true,
      "stacked": false
     },
     "type": "bar",
     "version": "v1"
    },
    "executionCancelledAt": 1744360323614,
    "executionTime": 57,
    "lastExecutedAt": 1744282850405,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "car_reviews.head()",
    "outputsMetadata": {
     "0": {
      "chartState": {
       "chartModel": {
        "cellRange": {
         "columns": [],
         "rowEndIndex": null,
         "rowStartIndex": null
        },
        "chartId": "eqibazd5ne9",
        "chartOptions": {
         "bar": {
          "axes": {},
          "legend": {
           "enabled": false
          }
         }
        },
        "chartType": "groupedColumn",
        "modelType": "range"
       },
       "conversionErrors": [
        "invalid-axis-config"
       ],
       "pivotMode": {
        "enabled": false
       },
       "rangeChartModel": {
        "rangeColumns": []
       },
       "sharedChartOptions": {
        "bar": {
         "axes": {},
         "legend": {
          "enabled": false
         }
        }
       }
      },
      "height": 550,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "3aaf9a68-bd66-4dae-b308-abf466af1349",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    },
    "version": "ag-charts-v1",
    "visualizeDataframe": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am very satisfied with my 2014 Nissan NV SL....</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The car is fine. It's a bit loud and not very ...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My first foreign car. Love it, I would buy ano...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've come across numerous reviews praising the...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been dreaming of owning an SUV for quite ...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review     Class\n",
       "0  I am very satisfied with my 2014 Nissan NV SL....  POSITIVE\n",
       "1  The car is fine. It's a bit loud and not very ...  NEGATIVE\n",
       "2  My first foreign car. Love it, I would buy ano...  POSITIVE\n",
       "3  I've come across numerous reviews praising the...  NEGATIVE\n",
       "4  I've been dreaming of owning an SUV for quite ...  POSITIVE"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb908b5d-2d9a-4fef-b929-70ad03094ed4",
   "metadata": {
    "chartConfig": {
     "bar": {
      "hasRoundedCorners": true,
      "stacked": false
     },
     "type": "line",
     "version": "v1"
    },
    "executionCancelledAt": 1744360323616,
    "executionTime": 18,
    "lastExecutedAt": 1744282853387,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "car_reviews.describe()",
    "outputsMetadata": {
     "0": {
      "chartState": {
       "chartModel": {
        "cellRange": {
         "columns": [],
         "rowEndIndex": null,
         "rowStartIndex": null
        },
        "chartId": "ers7lcnn13",
        "chartOptions": {
         "line": {
          "axes": {},
          "legend": {
           "enabled": false
          }
         }
        },
        "chartType": "line",
        "modelType": "range"
       },
       "conversionErrors": [
        "invalid-axis-config"
       ],
       "pivotMode": {
        "enabled": false
       },
       "rangeChartModel": {
        "rangeColumns": []
       },
       "sharedChartOptions": {
        "line": {
         "axes": {},
         "legend": {
          "enabled": false
         }
        }
       }
      },
      "height": 550,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "3aaf9a68-bd66-4dae-b308-abf466af1349",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    },
    "version": "ag-charts-v1",
    "visualizeDataframe": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>I am very satisfied with my 2014 Nissan NV SL....</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Review     Class\n",
       "count                                                   5         5\n",
       "unique                                                  5         2\n",
       "top     I am very satisfied with my 2014 Nissan NV SL....  POSITIVE\n",
       "freq                                                    1         3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb294ea-d4e7-4dd2-be5e-5b965cc9851e",
   "metadata": {
    "executionCancelledAt": 1744360323617,
    "executionTime": 46,
    "lastExecutedAt": 1744283123724,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Select the review column and the real labels\nreviews = car_reviews['Review'].tolist()\nreal_labels = car_reviews['Class'].tolist()",
    "outputsMetadata": {
     "0": {
      "height": 550,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "43a19fe1-1ec9-4aac-bdc6-0dd335f7112f",
        "nodeType": "const"
       },
       "quickFilterText": ""
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Select the review column and the real labels\n",
    "reviews = car_reviews['Review'].tolist()\n",
    "real_labels = car_reviews['Class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857c9b6f-e4db-454c-a1f6-afda211a15c3",
   "metadata": {
    "executionCancelledAt": 1744360323619,
    "executionTime": 9,
    "lastExecutedAt": 1744283128950,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "print(reviews[:2],\"#\"* 100, real_labels[0:2])",
    "outputsMetadata": {
     "0": {
      "height": 479,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.', \"The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\"] #################################################################################################### ['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:2],\"#\"* 100, real_labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab3105-5a2e-4125-9f30-06e614a08b9f",
   "metadata": {},
   "source": [
    "## Sentiment classification with an off the shelf LLM and BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8236698f-7bdf-474e-b9f2-32aa68fa14e1",
   "metadata": {},
   "source": [
    "Some issues you may encounter is using up all your all you credits to do this task. The second method is more realiable since you are bringing the model to the workspace. In addition, it is better getting the model information from the model pages for example https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct then checking out the inference provider. Ollama could be another good alternative. But, I recommend using between 0.5B-7B models in your computer if you have at least 8B of VRAM. Alternatively, you could change a bit of the above API to use openAI models instead. Do what you are comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "873bac3b-b4a6-4b0c-9c63-435c43a62420",
   "metadata": {
    "executionCancelledAt": 1744360323620,
    "executionTime": 17219,
    "lastExecutedAt": 1744281779190,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def format_input(text):\n    \"\"\"Format the input text for sentiment analysis\"\"\"\n    return f\"Analyze the sentiment of this text: '{text}'\"\n\n# Define system prompt for sentiment analysis\nsystem_prompt = \"\"\"You are a sentiment analysis model. Follow these rules strictly:\n1. If the review expresses positive feelings, satisfaction, praise, or approval, respond with: POSITIVE\n2. If the review expresses negative feelings, complaints, criticism, or disapproval, respond with: NEGATIVE\n3. Include a confidence score between 0 and 1 in parentheses after your sentiment label, e.g. \"POSITIVE (0.95)\"\n4. Base your analysis on the emotional tone and content of the text\"\"\"\n\n# Iterate through reviews and get sentiment scores\nif \"Review\" in car_reviews.columns:\n    # Create the sentiment columns if they don't exist\n    if 'predicted_labels' not in car_reviews.columns:\n        car_reviews['predicted_labels'] = None\n    if 'confidence' not in car_reviews.columns:\n        car_reviews['confidence'] = None\n    \n    for index, review_text in car_reviews[\"Review\"].items():\n        user_prompt = format_input(review_text)\n        print(\"#\" * 100)\n        print(f\"Review: {review_text}\")\n        try:\n            result = run_hfgpt(system_prompt, user_prompt, client)\n            \n            # Parse the result to extract sentiment and confidence\n            # Expected format: \"POSITIVE (0.9294)\" or \"NEGATIVE (0.8654)\"\n            parts = result.strip().split('(')\n            sentiment = parts[0].strip()\n            confidence = float(parts[1].strip(')')) if len(parts) > 1 else None\n            \n            print(f\"Review {index}: {sentiment} (Confidence: {confidence})\")\n            \n            # Store sentiment and confidence separately\n            car_reviews.at[index, 'predicted_labels'] = sentiment\n            car_reviews.at[index, 'confidence'] = confidence\n            \n            # Print formatted output\n            if 'Actual_Sentiment' in car_reviews.columns:\n                actual = car_reviews.at[index, 'Actual_Sentiment']\n                print(f\"Actual Sentiment: {actual}\")\n            print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n            print(\"#\" * 100)\n            \n        except Exception as e:\n            print(f\"Error processing review {index}: {e}\")\n            car_reviews.at[index, 'predicted_labels'] = None\n            car_reviews.at[index, 'confidence'] = None\nelse:\n    print(f\"Column 'Review' not found\")\n\n# Function to display formatted results (optional)\ndef display_sentiment_analysis(df):\n    for index, row in df.iterrows():\n        if 'Review' in row and 'predicted_labels' in row and 'confidence' in row:\n            print(f\"Review: {row['Review']}\")\n            if 'Actual_Sentiment' in row:\n                print(f\"Actual Sentiment: {row['Actual_Sentiment']}\")\n            print(f\"Predicted Sentiment: {row['predicted_labels']} (Confidence: {row['confidence']:.4f})\")\n            print()\n\n# To display the results in the desired format\ndisplay_sentiment_analysis(car_reviews)",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "Review: I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Review 0: POSITIVE (Confidence: 0.98)\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9800)\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Review: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Review 1: NEGATIVE (Confidence: 0.95)\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9500)\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Review: My first foreign car. Love it, I would buy another.\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Review 2: POSITIVE (Confidence: 0.95)\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9500)\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Review: I've come across numerous reviews praising the Rogue, and I genuinely feel like I might be missing something. It's only been a week since I got the car, and I am genuinely disappointed. I truly wish I could return it. My main concern revolves around what I see as a significant design flaw (which I believe also exists in the Murano, though that wasn't much better and considerably pricier). The rear windshield is just too small. The headrests in the back seat obstruct the sides of the rearview window. This \"Crossover\" feels more like a cheaply made compact car. My other vehicle is a Sonata, and it provides a significantly quieter and smoother ride. I did not anticipate this car to ride so roughly; my 2006 Pathfinder had a smoother ride! I would rate this car a 5 all around.\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Review 3: NEGATIVE (Confidence: 0.98)\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9800)\n",
      "####################################################################################################\n",
      "####################################################################################################\n",
      "Review: I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Review 4: POSITIVE (Confidence: 0.98)\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9800)\n",
      "####################################################################################################\n",
      "Review: I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9800)\n",
      "\n",
      "Review: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9500)\n",
      "\n",
      "Review: My first foreign car. Love it, I would buy another.\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9500)\n",
      "\n",
      "Review: I've come across numerous reviews praising the Rogue, and I genuinely feel like I might be missing something. It's only been a week since I got the car, and I am genuinely disappointed. I truly wish I could return it. My main concern revolves around what I see as a significant design flaw (which I believe also exists in the Murano, though that wasn't much better and considerably pricier). The rear windshield is just too small. The headrests in the back seat obstruct the sides of the rearview window. This \"Crossover\" feels more like a cheaply made compact car. My other vehicle is a Sonata, and it provides a significantly quieter and smoother ride. I did not anticipate this car to ride so roughly; my 2006 Pathfinder had a smoother ride! I would rate this car a 5 all around.\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9800)\n",
      "\n",
      "Review: I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9800)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_input(text):\n",
    "    \"\"\"Format the input text for sentiment analysis\"\"\"\n",
    "    return f\"Analyze the sentiment of this text: '{text}'\"\n",
    "\n",
    "# Define system prompt for sentiment analysis\n",
    "system_prompt = \"\"\"You are a sentiment analysis model. Follow these rules strictly:\n",
    "1. If the review expresses positive feelings, satisfaction, praise, or approval, respond with: POSITIVE\n",
    "2. If the review expresses negative feelings, complaints, criticism, or disapproval, respond with: NEGATIVE\n",
    "3. Include a confidence score between 0 and 1 in parentheses after your sentiment label, e.g. \"POSITIVE (0.95)\"\n",
    "4. Base your analysis on the emotional tone and content of the text\"\"\"\n",
    "\n",
    "# Iterate through reviews and get sentiment scores\n",
    "if \"Review\" in car_reviews.columns:\n",
    "    # Create the sentiment columns if they don't exist\n",
    "    if 'predicted_labels' not in car_reviews.columns:\n",
    "        car_reviews['predicted_labels'] = None\n",
    "    if 'confidence' not in car_reviews.columns:\n",
    "        car_reviews['confidence'] = None\n",
    "\n",
    "    for index, review_text in car_reviews[\"Review\"].items():\n",
    "        user_prompt = format_input(review_text)\n",
    "        print(\"#\" * 100)\n",
    "        print(f\"Review: {review_text}\")\n",
    "        try:\n",
    "            result = run_hfgpt(system_prompt, user_prompt, client)\n",
    "\n",
    "            # Parse the result to extract sentiment and confidence\n",
    "            # Expected format: \"POSITIVE (0.9294)\" or \"NEGATIVE (0.8654)\"\n",
    "            parts = result.strip().split('(')\n",
    "            sentiment = parts[0].strip()\n",
    "            confidence = float(parts[1].strip(')')) if len(parts) > 1 else None\n",
    "\n",
    "            print(f\"Review {index}: {sentiment} (Confidence: {confidence})\")\n",
    "\n",
    "            # Store sentiment and confidence separately\n",
    "            car_reviews.at[index, 'predicted_labels'] = sentiment\n",
    "            car_reviews.at[index, 'confidence'] = confidence\n",
    "\n",
    "            # Print formatted output\n",
    "            if 'Actual_Sentiment' in car_reviews.columns:\n",
    "                actual = car_reviews.at[index, 'Actual_Sentiment']\n",
    "                print(f\"Actual Sentiment: {actual}\")\n",
    "            print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "            print(\"#\" * 100)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing review {index}: {e}\")\n",
    "            car_reviews.at[index, 'predicted_labels'] = None\n",
    "            car_reviews.at[index, 'confidence'] = None\n",
    "else:\n",
    "    print(f\"Column 'Review' not found\")\n",
    "\n",
    "# Function to display formatted results (optional)\n",
    "def display_sentiment_analysis(df):\n",
    "    for index, row in df.iterrows():\n",
    "        if 'Review' in row and 'predicted_labels' in row and 'confidence' in row:\n",
    "            print(f\"Review: {row['Review']}\")\n",
    "            if 'Actual_Sentiment' in row:\n",
    "                print(f\"Actual Sentiment: {row['Actual_Sentiment']}\")\n",
    "            print(f\"Predicted Sentiment: {row['predicted_labels']} (Confidence: {row['confidence']:.4f})\")\n",
    "            print()\n",
    "\n",
    "# To display the results in the desired format\n",
    "display_sentiment_analysis(car_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e94b2ff-756c-42e9-a627-a40248379493",
   "metadata": {
    "executionCancelledAt": 1744360323621,
    "executionTime": 649,
    "lastExecutedAt": 1744281779839,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load a sentiment analysis LLM into a pipeline\nfrom transformers import pipeline\nclassifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n\n# Perform inference on the car reviews and display prediction results\npredicted_labels = classifier(reviews)\nfor review, prediction, label in zip(reviews, predicted_labels, real_labels):\n    print(f\"Review: {review}\\nActual Sentiment: {label}\\nPredicted Sentiment: {prediction['label']} (Confidence: {prediction['score']:.4f})\\n\")\n",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     },
     "1": {
      "height": 616,
      "type": "stream"
     },
     "4": {
      "height": 39,
      "type": "stream"
     },
     "5": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I am very satisfied with my 2014 Nissan NV SL. I use this van for my business deliveries and personal use. Camping, road trips, etc. We dont have any children so I store most of the seats in my warehouse. I wanted the passenger van for the rear air conditioning. We drove our van from Florida to California for a Cross Country trip in 2014. We averaged about 18 mpg. We drove thru a lot of rain and It was a very comfortable and stable vehicle. The V8 Nissan Titan engine is a 500k mile engine. It has been tested many times by delivery and trucking companies. This is why Nissan gives you a 5 year or 100k mile bumper to bumper warranty. Many people are scared about driving this van because of its size. But with front and rear sonar sensors, large mirrors and the back up camera. It is easy to drive. The front and rear sensors also monitor the front and rear sides of the bumpers making it easier to park close to objects. Our Nissan NV is a Tow Monster. It pulls our 5000 pound travel trailer like its not even there. I have plenty of power to pass a vehicle if needed. The 5.6 liter engine produces 317 hp. I have owned Chevy and Ford vans and there were not very comfortable and had little cockpit room. The Nissan NV is the only vehicle made that has the engine forward like a pick up truck giving the driver plenty of room and comfort in the cockpit area. I dont have any negatives to say about my NV. This is a wide vehicle. The only modification I would like to see from Nissan is for them to add amber side mirror marker lights.BTW. I now own a 2016 Nissan NVP SL. Love it.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9294)\n",
      "\n",
      "Review: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.8654)\n",
      "\n",
      "Review: My first foreign car. Love it, I would buy another.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9995)\n",
      "\n",
      "Review: I've come across numerous reviews praising the Rogue, and I genuinely feel like I might be missing something. It's only been a week since I got the car, and I am genuinely disappointed. I truly wish I could return it. My main concern revolves around what I see as a significant design flaw (which I believe also exists in the Murano, though that wasn't much better and considerably pricier). The rear windshield is just too small. The headrests in the back seat obstruct the sides of the rearview window. This \"Crossover\" feels more like a cheaply made compact car. My other vehicle is a Sonata, and it provides a significantly quieter and smoother ride. I did not anticipate this car to ride so roughly; my 2006 Pathfinder had a smoother ride! I would rate this car a 5 all around.\n",
      "Actual Sentiment: NEGATIVE\n",
      "Predicted Sentiment: NEGATIVE (Confidence: 0.9935)\n",
      "\n",
      "Review: I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n",
      "Actual Sentiment: POSITIVE\n",
      "Predicted Sentiment: POSITIVE (Confidence: 0.9987)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a sentiment analysis LLM into a pipeline\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Perform inference on the car reviews and display prediction results\n",
    "predicted_labels = classifier(reviews)\n",
    "for review, prediction, label in zip(reviews, predicted_labels, real_labels):\n",
    "    print(f\"Review: {review}\\nActual Sentiment: {label}\\nPredicted Sentiment: {prediction['label']} (Confidence: {prediction['score']:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4156fa-609c-4867-b392-e59e4172abaf",
   "metadata": {},
   "source": [
    "Seems that the now popular causal language models are giving the same kind of sentiment scores with more conservative with the BERT models which is a encoder model; it is good at Natural Language Understanding. That means, it compresses information into a numerical representation and a score is given. Where as the decoder which excels at Natural language generation seems to work to upon aligning it to a system prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ae454-0e77-4de3-87e2-86e8e51de45b",
   "metadata": {},
   "source": [
    "This is important while doing LLM evaluations. Then we can track this in a database or csvfile. Probably the latter to make things lite weight. But, if its for school or work I think a database would be better. Let's see if the LLM did the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66a09a1d-4dbc-47ce-aaf8-f1f3d7b28ddc",
   "metadata": {
    "executionCancelledAt": 1744360323623,
    "executionTime": 47,
    "lastExecutedAt": 1744281942557,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "real_labels"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a83b11-a81d-4697-a0be-725c13dbcabe",
   "metadata": {
    "executionCancelledAt": 1744360323624,
    "executionTime": 728,
    "lastExecutedAt": 1744281947892,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Load accuracy and F1 score metrics    \nimport evaluate\naccuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\n# Map categorical sentiment labels into integer labels\nreferences = [1 if label == \"POSITIVE\" else 0 for label in real_labels]\npredictions = [1 if label['label'] == \"POSITIVE\" else 0 for label in predicted_labels]\n\n# Calculate accuracy and F1 score\naccuracy_result_dict = accuracy.compute(references=references, predictions=predictions)\naccuracy_result = accuracy_result_dict['accuracy']\nf1_result_dict = f1.compute(references=references, predictions=predictions)\nf1_result = f1_result_dict['f1']\nprint(f\"Accuracy: {accuracy_result}\")\nprint(f\"F1 result: {f1_result}\")\n",
    "outputsMetadata": {
     "2": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "F1 result: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Load accuracy and F1 score metrics\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Map categorical sentiment labels into integer labels\n",
    "references = [1 if label == \"POSITIVE\" else 0 for label in real_labels]\n",
    "predictions = [1 if label['label'] == \"POSITIVE\" else 0 for label in predicted_labels]\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy_result_dict = accuracy.compute(references=references, predictions=predictions)\n",
    "accuracy_result = accuracy_result_dict['accuracy']\n",
    "\n",
    "# Fix for f1 calculation - handle the result safely whether it's a float or array\n",
    "from sklearn.metrics import f1_score\n",
    "f1_result = f1_score(references, predictions)\n",
    "print(f\"Accuracy: {accuracy_result}\")\n",
    "print(f\"F1 result: {f1_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eeb9a3-e93a-46fb-999d-2f1ffb31b0c3",
   "metadata": {
    "executionCancelledAt": 1744360323625,
    "executionTime": 22,
    "lastExecutedAt": 1744281958716,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "car_reviews",
    "outputsMetadata": {
     "0": {
      "height": 50,
      "tableState": {
       "customFilter": {
        "const": {
         "type": "boolean",
         "valid": true,
         "value": true
        },
        "id": "81b0ac8f-cdfb-498e-b6e7-d9eb9dd0d48c",
        "nodeType": "const"
       }
      },
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "car_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37440de7-e8da-452c-8608-36e641e6b5aa",
   "metadata": {
    "executionCancelledAt": 1744360323626,
    "executionTime": 15,
    "lastExecutedAt": 1744281963048,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "def calculate_metrics(df):\n    \"\"\"\n    Calculate F1-score and accuracy using NumPy operations.\n    \n    Args:\n        df: DataFrame with 'Class' and 'predictions' columns\n        \n    Returns:\n        DataFrame with added metric columns\n    \"\"\"\n    # Convert 'Class' to binary (0/1)\n    true_labels = np.where(car_reviews['Class'] == 'POSITIVE', 1, 0)\n    predictions = np.where(car_reviews['predicted_labels'] == \"POSITIVE\", 1, 0)\n    \n    # Calculate True Positives, False Positives, False Negatives\n    tp = np.sum((true_labels == 1) & (predictions == 1))\n    fp = np.sum((true_labels == 0) & (predictions == 1))\n    fn = np.sum((true_labels == 1) & (predictions == 0))\n    \n    # Calculate precision and recall\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n    \n    # Calculate F1-score\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    # Calculate accuracy\n    accuracy = np.mean(true_labels == predictions)\n    \n    # Add metrics as new columns\n    df['accuracy_result'] = accuracy\n    df['f1_result'] = f1\n    \n    return df\n\n# Apply the metrics calculation\ncar_reviews = calculate_metrics(car_reviews)\n\n# Print metrics to verify\nprint(f\"F1-Score: {car_reviews['f1_result'].iloc[0]:.3f}\")\nprint(f\"Accuracy: {car_reviews['accuracy_result'].iloc[0]:.3f}\")",
    "outputsMetadata": {
     "0": {
      "height": 59,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score: 1.000\n",
      "Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate F1-score and accuracy using NumPy operations.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with 'Class' and 'predictions' columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with added metric columns\n",
    "    \"\"\"\n",
    "    # Convert 'Class' to binary (0/1)\n",
    "    true_labels = np.where(car_reviews['Class'] == 'POSITIVE', 1, 0)\n",
    "    predictions = np.where(car_reviews['predicted_labels'] == \"POSITIVE\", 1, 0)\n",
    "\n",
    "    # Calculate True Positives, False Positives, False Negatives\n",
    "    tp = np.sum((true_labels == 1) & (predictions == 1))\n",
    "    fp = np.sum((true_labels == 0) & (predictions == 1))\n",
    "    fn = np.sum((true_labels == 1) & (predictions == 0))\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    # Calculate F1-score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(true_labels == predictions)\n",
    "\n",
    "    # Add metrics as new columns\n",
    "    df['accuracy_result'] = accuracy\n",
    "    df['f1_result'] = f1\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the metrics calculation\n",
    "car_reviews = calculate_metrics(car_reviews)\n",
    "\n",
    "# Print metrics to verify\n",
    "print(f\"F1-Score: {car_reviews['f1_result'].iloc[0]:.3f}\")\n",
    "print(f\"Accuracy: {car_reviews['accuracy_result'].iloc[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb1dcc-02e6-42b6-922c-be55340a642e",
   "metadata": {},
   "source": [
    "I would trust the output of the previous model more. Sometimes you get the same result as what you were aiming for in this situation the true_labels with the predicted labels. Making this a classic classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c40b3d4-e918-42ad-bf5a-072e3918c19e",
   "metadata": {},
   "source": [
    "# Task 2: Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836bf2b-5f3f-43df-9c98-0cebb6e11678",
   "metadata": {},
   "source": [
    "Looking good so far the reviews matching the original classes. But, it is never this easy for real world data. Now, let's look into a translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db696365-e3ca-46f4-9b28-133b58df46cb",
   "metadata": {
    "executionCancelledAt": 1744360323627,
    "executionTime": 10431,
    "lastExecutedAt": 1744281981505,
    "lastExecutedByKernel": "4b91eae4-60de-4c1c-866f-9a5019ca15ea",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# reuse the previous format text function \nsystem_prompt = \"\"\"You are an expert Spanish-English translator. Provide accurate translations while preserving complete meaning.\n\nExamples:\n\nInput: \"El tiempo es oro.\"\nTranslation: \"Time is gold.\"\nContext: Direct equivalent of \"time is money\" saying\n\nInput: \"Estar entre la espada y la pared\"\nTranslation: \"To be between the sword and the wall\"  \nContext: Equivalent to \"between a rock and a hard place\"\n\nInput: \"Cada loco con su tema\"  \nTranslation: \"Each crazy person with their theme\"\nContext: Similar to \"to each their own\"\n\nInstructions:\n- Maintain original meaning\n- Preserve tone and register\n- Keep cultural context\n- Include all source information\n\"\"\"\n\n# Create list outside the loop to store unique translations\ntranslated_review = []\nseen_sentences = set()  # Track unique sentences\n\n# Read the file content\ndef calculate_bleu(reference, candidate):\n    \"\"\"\n    Calculate a simplified BLEU score between reference and candidate translations.\n    Uses unigram precision (word-level matching).\n    \n    Args:\n        reference: Reference (ground truth) translation string\n        candidate: Candidate (model) translation string\n    Returns:\n        Float between 0-1 representing translation quality\n    \"\"\"\n    # Tokenize into words (simple split on spaces)\n    ref_words = reference.lower().split()\n    cand_words = candidate.lower().split()\n    \n    # Count matching words\n    matches = sum(1 for word in cand_words if word in ref_words)\n    \n    # Calculate precision\n    precision = matches / len(cand_words) if len(cand_words) > 0 else 0\n    \n    # Length penalty if candidate is too short\n    brevity_penalty = min(1.0, len(cand_words) / len(ref_words)) if len(ref_words) > 0 else 0\n    \n    # Final BLEU score\n    bleu = precision * brevity_penalty\n    return bleu\n\n# Use in translation loop\ntranslated_review = []\nseen_sentences = set()\nbleu_scores = []\n\nwith open('data/reference_translations.txt', 'r', encoding='utf-8') as file:\n    content = file.read()\n    sentences = content.split('.')\n\nfor sentence in sentences:\n    sentence = sentence.strip()\n    if sentence and sentence not in seen_sentences:\n        seen_sentences.add(sentence)\n        user_prompt = f\"Translate this Spanish text to English: {sentence}\"\n        try:\n            translation = run_hfgpt(system_prompt, user_prompt, client)\n            score = calculate_bleu(sentence, translation)\n            bleu_scores.append(score)\n            translated_review.append(translation)\n            \n            print(f\"Original: {sentence}\")\n            print(f\"Translation: {translation}\")\n            print(f\"BLEU Score: {score:.4f}\\n\")\n            \n        except Exception as e:\n            print(f\"Error: {e}\")\n            continue\n\nprint(f\"Average BLEU score: {sum(bleu_scores)/len(bleu_scores)*.01 :.4f}\")",
    "outputsMetadata": {
     "0": {
      "height": 353,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Original: Estoy muy satisfecho con mi Nissan NV SL 2014\n",
      "Translation: I am very satisfied with my 2014 Nissan NV SL.\n",
      "BLEU Score: 0.3000\n",
      "\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Original: Estoy muy satisfecho con mi Nissan NV SL 2014\n",
      "Translation: I am very satisfied with my 2014 Nissan NV SL.\n",
      "BLEU Score: 0.3000\n",
      "\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Original: Utilizo esta camioneta para mis entregas comerciales y uso personal\n",
      "Translation: I use this truck for both my commercial deliveries and personal use.\n",
      "BLEU Score: 0.0833\n",
      "\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Original: Utilizo esta camioneta para mis entregas comerciales y uso personal\n",
      "Translation: I use this truck for both my commercial deliveries and personal use.\n",
      "BLEU Score: 0.0833\n",
      "\n",
      "Sending request to model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Original: Uso esta furgoneta para mis entregas comerciales y uso personal\n",
      "Translation: I use this truck for both my commercial deliveries and personal use.\n",
      "BLEU Score: 0.0833\n",
      "\n",
      "Average BLEU score: 0.0016\n",
      "Original: Uso esta furgoneta para mis entregas comerciales y uso personal\n",
      "Translation: I use this truck for both my commercial deliveries and personal use.\n",
      "BLEU Score: 0.0833\n",
      "\n",
      "Average BLEU score: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# reuse the previous format text function\n",
    "system_prompt = \"\"\"You are an expert Spanish-English translator. Provide accurate translations while preserving complete meaning.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Input: \"El tiempo es oro.\"\n",
    "Translation: \"Time is gold.\"\n",
    "Context: Direct equivalent of \"time is money\" saying\n",
    "\n",
    "Input: \"Estar entre la espada y la pared\"\n",
    "Translation: \"To be between the sword and the wall\"\n",
    "Context: Equivalent to \"between a rock and a hard place\"\n",
    "\n",
    "Input: \"Cada loco con su tema\"\n",
    "Translation: \"Each crazy person with their theme\"\n",
    "Context: Similar to \"to each their own\"\n",
    "\n",
    "Instructions:\n",
    "- Maintain original meaning\n",
    "- Preserve tone and register\n",
    "- Keep cultural context\n",
    "- Include all source information\n",
    "\"\"\"\n",
    "\n",
    "# Create list outside the loop to store unique translations\n",
    "translated_review = []\n",
    "seen_sentences = set()  # Track unique sentences\n",
    "\n",
    "# Read the file content\n",
    "def calculate_bleu(reference, candidate):\n",
    "    \"\"\"\n",
    "    Calculate a simplified BLEU score between reference and candidate translations.\n",
    "    Uses unigram precision (word-level matching).\n",
    "\n",
    "    Args:\n",
    "        reference: Reference (ground truth) translation string\n",
    "        candidate: Candidate (model) translation string\n",
    "    Returns:\n",
    "        Float between 0-1 representing translation quality\n",
    "    \"\"\"\n",
    "    # Tokenize into words (simple split on spaces)\n",
    "    ref_words = reference.lower().split()\n",
    "    cand_words = candidate.lower().split()\n",
    "\n",
    "    # Count matching words\n",
    "    matches = sum(1 for word in cand_words if word in ref_words)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = matches / len(cand_words) if len(cand_words) > 0 else 0\n",
    "\n",
    "    # Length penalty if candidate is too short\n",
    "    brevity_penalty = min(1.0, len(cand_words) / len(ref_words)) if len(ref_words) > 0 else 0\n",
    "\n",
    "    # Final BLEU score\n",
    "    bleu = precision * brevity_penalty\n",
    "    return bleu\n",
    "\n",
    "# Use in translation loop\n",
    "translated_review = []\n",
    "seen_sentences = set()\n",
    "bleu_scores = []\n",
    "\n",
    "with open('data/reference_translations.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    sentences = content.split('.')\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    if sentence and sentence not in seen_sentences:\n",
    "        seen_sentences.add(sentence)\n",
    "        user_prompt = f\"Translate this Spanish text to English: {sentence}\"\n",
    "        try:\n",
    "            translation = run_hfgpt(system_prompt, user_prompt, client)\n",
    "            score = calculate_bleu(sentence, translation)\n",
    "            bleu_scores.append(score)\n",
    "            translated_review.append(translation)\n",
    "\n",
    "            print(f\"Original: {sentence}\")\n",
    "            print(f\"Translation: {translation}\")\n",
    "            print(f\"BLEU Score: {score:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"Average BLEU score: {sum(bleu_scores)/len(bleu_scores)*.01 :.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40799cba-14b2-4383-8dfe-83de4dfd3afd",
   "metadata": {},
   "source": [
    "The second translation the model gave a prediction of truck instead of a van. I think when doing translations especially with models like these ones it might help to read the model cards more. Learning more about the support of the language of interest. Let's see what variables we have so far! You can use `%whos` don't do this publicly. It is a good way to see what variables you have in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b472a6-3c60-4bc4-a9bc-112fd0f9f3ae",
   "metadata": {},
   "source": [
    "Let's try translating with a different model more attuned to translation from the `Helsinki-NLP` group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95801c7b-e293-45bf-908e-d4c10ef3a1a4",
   "metadata": {
    "executionCancelledAt": 1744360323629,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 416,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your input_length: 365 is bigger than 0.9 * max_length: 27. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n",
      "Your input_length: 365 is bigger than 0.9 * max_length: 27. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model translation:\n",
      "Estoy muy satisfecho con mi 2014 Nissan NV SL. Uso esta furgoneta para mis entregas de negocios y uso personal.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gc\n",
    "\n",
    "# 1. Load the model with memory optimizations for CPU\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-es\",\n",
    "    device=-1,  # Explicitly specify CPU\n",
    "    model_kwargs={\"low_cpu_mem_usage\": True}  # Reduces memory footprint during loading\n",
    ")\n",
    "\n",
    "# 2. Process just the first review with CPU-friendly parameters\n",
    "first_review = reviews[0]\n",
    "translated_review = translator(\n",
    "    first_review,\n",
    "    max_length=27,\n",
    "    batch_size=1  # Process one sample at a time\n",
    ")[0]['translation_text']\n",
    "\n",
    "print(f\"Model translation:\\n{translated_review}\")\n",
    "\n",
    "# 3. Clean up\n",
    "del translator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e523739e-fae4-430f-9e5c-e0c87f32e879",
   "metadata": {
    "executionCancelledAt": 1744360323632
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish translation references:\n",
      "['Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.', 'Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta furgoneta para mis entregas comerciales y uso personal.']\n"
     ]
    }
   ],
   "source": [
    "# Load reference translations from file\n",
    "with open(\"data/reference_translations.txt\", 'r') as file:\n",
    "    lines = file.readlines()\n",
    "references = [line.strip() for line in lines]\n",
    "print(f\"Spanish translation references:\\n{references}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11248df8-1064-474b-b305-ab76e44354e9",
   "metadata": {},
   "source": [
    "The translations were not so bad. The reviews were under 0.5. Now let's see what we can learn from a extractive QA LLM about the second review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "754c47c9-53bd-4404-a779-e5ea632a5a86",
   "metadata": {
    "executionCancelledAt": 1744360323634,
    "executionTime": 50,
    "lastExecutedAt": 1742986638152,
    "lastExecutedByKernel": "53e78938-378d-416b-90db-0371965b3778",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "translated_review[1]"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Estoy muy satisfecho con mi 2014 Nissan NV SL. Uso esta furgoneta para mis entregas de negocios y uso personal.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d9c16-955e-4ac2-963a-eb9d9a823c1f",
   "metadata": {
    "executionCancelledAt": 1744360323635,
    "executionTime": 414,
    "lastExecutedAt": 1744282042129,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "#https://huggingface.co/deepset/minilm-uncased-squad2\n# thanks for the documentation let's go\nmodel_name = \"deepset/minilm-uncased-squad2\"\n\n# a) Get predictions\nnlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\nQA_input = {\n    'question': \"What did he like about the brand?\",\n    'context': translated_review[1]\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprint(res.get(\"answer\"))",
    "outputsMetadata": {
     "0": {
      "height": 290,
      "type": "stream"
     },
     "1": {
      "height": 38,
      "type": "stream"
     },
     "3": {
      "height": 143,
      "type": "stream"
     },
     "7": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  ride quality, reliability\n"
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/deepset/minilm-uncased-squad2\n",
    "# thanks for the documentation let's go\n",
    "model_name = \"deepset/minilm-uncased-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': \"What did he like about the brand?\",\n",
    "    'context': translated_review[1]\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Perform inference and extract answer from raw outputs\n",
    "with torch.no_grad():\n",
    "  outputs = model(**inputs)\n",
    "start_idx = torch.argmax(outputs.start_logits)\n",
    "end_idx = torch.argmax(outputs.end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "\n",
    "# Decode and show answer\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20c275-ce8e-45d1-9bec-ee5b3d4b15cb",
   "metadata": {},
   "source": [
    "## Task: Summarize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d2c0cf6-d5c7-4227-94c3-db4fe47b89c7",
   "metadata": {
    "executionCancelledAt": 1744360323636,
    "executionTime": 51,
    "lastExecutedAt": 1744282079056,
    "lastExecutedByKernel": "a83e30e5-fd36-4a35-a19d-3fd781df1d45",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Get original text to summarize upon car review\ntext_to_summarize = reviews[-1]\nprint(f\"Original text:\\n{text_to_summarize}\")\n\n# Load summarization pipeline and perform inference\nmodel_name = \"cnicu/t5-small-booksum\"\nsummarizer = pipeline(\"summarization\", model=model_name)\noutputs = summarizer(text_to_summarize, max_length=53)\nsummarized_text = outputs[0]['summary_text']\nprint(f\"Summarized text:\\n{summarized_text}\")\n",
    "outputsMetadata": {
     "0": {
      "height": 227,
      "type": "stream"
     },
     "8": {
      "height": 38,
      "type": "stream"
     },
     "9": {
      "height": 101,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text:\n",
      "the Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. I have hauled 12 bags of mulch in the back with the seats down and could have held more.\n"
     ]
    }
   ],
   "source": [
    "# Get original text to summarize upon car review\n",
    "text_to_summarize = reviews[-1]\n",
    "print(f\"Original text:\\n{text_to_summarize}\")\n",
    "\n",
    "# Load summarization pipeline and perform inference\n",
    "model_name = \"cnicu/t5-small-booksum\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "outputs = summarizer(text_to_summarize, max_length=53)\n",
    "summarized_text = outputs[0]['summary_text']\n",
    "print(f\"Summarized text:\\n{summarized_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "417958fa-89e9-44de-b53a-c8037e627ad5",
   "metadata": {
    "executionCancelledAt": 1744360323638,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 227,
      "type": "stream"
     },
     "7": {
      "height": 59,
      "type": "stream"
     },
     "8": {
      "height": 80,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text:\n",
      "The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. The engine delivers strong performance, and the ride is really smooth.\n"
     ]
    }
   ],
   "source": [
    "# Using a model I found on HF\n",
    "# Ranked the highest for summarization\n",
    "# Get original text to summarize upon car review\n",
    "text_to_summarize = reviews[-1]\n",
    "print(f\"Original text:\\n{text_to_summarize}\")\n",
    "\n",
    "# Load summarization pipeline and perform inference\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "outputs = summarizer(text_to_summarize, max_length=65)\n",
    "summarized_text = outputs[0]['summary_text']\n",
    "print(f\"Summarized text:\\n{summarized_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49898f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
